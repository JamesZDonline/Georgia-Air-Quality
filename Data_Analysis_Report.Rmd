---
title: "Georgia Air Quality Trends: 2005-2015"
output: html_document
---



##A brief word about the software.

The software used in the data cleaning and analysis and the production of these graphics is R, a powerful open source statistacal software package that is rapidly becoming a standard for statistical analysis. It has the benefits of reproducibility as all analyses are scripted, so it is easy to replicate exactly the steps taken on a set of data, and even to quickly apply the same steps to new data. It is also highly flexible, with many addon packages available and maintained to aid in analysis and visualization. For more informatoni about R see <http://www.r-project.org/>. 

This report was generated using R Markdown and the Knitter Package which allows the code used in the analysis to be easily integrated directly into the report. Using the R Studio development environment and the markdown document. It is possible to run the code directly in the document.

The additional packages used in this research are 

```{r,message=FALSE}
require(plyr)
require(zoo)
require(ggplot2)
```

##Data Acquisition Initial Cleaning

The data for this analysis, including observations of Ozone, Nitrogen Dioxide, Sulfer Dioxide, Lead and PM 2.5 over the last ten years was provided by Janet Aldredge, the Unit Manager of the Data Analysis Unit of EPD's Air Protection Branch Ambient Monitoring Program.
It was provided in a series of text delimeted files with a "vertical line" as the delimeter. The data also came with two header types, one for RC data and one for RD data and a comment at the end detailing how many obserations were included. The second header and the comment had to be removed for analysis.  

The following code reads in each of the data files, removes the second header and the comment, and adds the data to a dataframe named QData.

```{r,eval=FALSE}

#Read in Data file names from "Data" folder
files <- dir("Data")

#Clear QData if it already exists to start with a blank slate
if(exists("QData")){rm(QData)}

#for every data file, read it in and add it to QData.
for (file in files){
   
   #Read in the data files, pasting Data and the filename together for the path name
   QData_tmp <- read.csv(paste("Data",file,sep="/"),sep="|",header=TRUE,skip=0,na.strings="")
   
   #Data comes with two headers. Remove the Second Header
   QData_tmp <- QData_tmp[-1,]
   
   #Data comes with a comment at the end saying how many
   #rows were written, remove that too
   lastrow<-nrow(QData_tmp)
   QData_tmp <- QData_tmp[-lastrow,]
   
   #Check if QData exists. If it doesn't, use tmp data to make QData. If it does, add tmp data to QData.
   if (exists("QData")){
      QData <- rbind(QData,QData_tmp)
      }else{
         QData <- QData_tmp
         }
   print(paste(file,"loaded",sep=" "))
}
```

The data files can be quite large and take quite some time to load this way. To speed loading for later analysis, the data was saved in an RData file format using the following code.

```{r, eval=FALSE}
save(QData,file="QData.RData")
rm(QData_tmp)
```

Once the data has been saved, it can be rapidly loaded using the following command.

```{r,cache=TRUE}
load("QData.RData")
```

###Handeling RC vs RD data
Most of the data comes in the RD format, but as previously mentioned, some is RC (composite data). In order to correctly store all of the data in the same format, it is necessary to arrange the RC data to match the RD format and complete the table. To begin this process, the following code subsets out the RC data and saves it in a separate dataframe. It then renames the columns appropriately for RC data.
```{r}
#Some of the data is composite data. Such data is labeled as RC in the X..RD column and should have different headers.
#One way to fix this is to subset the data out, fix it and then merge it back in.
RCData<-which(QData$X..RD=="RC")
QDataRC<-QData[RCData,]

#Remove the RCData from the original Dataset
QData<-QData[-RCData,]

#Change headers for RC data to correct headers
RCheaders<-c("X..RD","Action.Code","State.Code","County.Code","Site.ID","Parameter",
             "POC","Unit","Method","Year","Month","Number.of.Samples","Composite.Type",
             "Sample.Value","Monitor.Protocol..MP..ID","Qualifier...1","Qualifier...2",
             "Qualifier...3","Qualifier...4","Qualifier...5","Qualifier...6","Qualifier...7",
             "Qualifier...8","Qualifier...9","Qualifier...10","Alternate.Method.Detectable.Limit",
             "Uncertainty","ExtraColumn")

names(QDataRC)<-RCheaders
```

The RD data table now contains Date and Start Time columns that are not included in the RC data table, as the RC data is data that was aggregated monthly. The following code fixes this by arbitrarily choosing the first of the month and 00:00 as the date and time used to represent the data.

```{r}
#Create Date Variable for RC Data. I decided to set the day of the month to be the first for each month. I can change this later
#if I find a need
QDataRC$Date<-paste(QDataRC$Year,QDataRC$Month,"01",sep="")

#Likewise arbitrarily set the start time to be midnight
QDataRC$Start.Time<-"00:00"
```

There are also several columns that occur in the composite data set, but not in the other which will not be relevant to this analysis. These can be removed.

```{r}
#Add null data code to be na
QDataRC$Null.Data.Code<-NA

#Composite Type is all Monthly so I can just remove that variable
QDataRC$Composite.Type<-NULL

QDataRC$ExtraColumn<-NULL
QDataRC$Year<-NULL
QDataRC$Month<-NULL
```

In order to re-merge the datasets and retain any columns that are not in both dataframes, the following function was taken from <https://amywhiteheadresearch.wordpress.com/2013/05/13/combining-dataframes-when-the-columns-dont-match/>

```{r}
rbind.all.columns <- function(x, y) {
   
   x.diff <- setdiff(colnames(x), colnames(y))
   y.diff <- setdiff(colnames(y), colnames(x))
   
   x[, c(as.character(y.diff))] <- NA
   
   y[, c(as.character(x.diff))] <- NA
   
   return(rbind(x, y))
}
```

This function was used to merge the RC and RD datasets into a single dataframe.

```{r,cache=TRUE}
#Recombine QData and QDataRC

QData<-rbind.all.columns(QData,QDataRC)
rm(QDataRC)
```

Finally, an examination of the data reveals that many of the columns in the QData dataframe do not contain any information, i.e. every observation was a NULL value.  These columns can be removed to reduce the memory useage and processing requirements.

```{r,cache=TRUE}
#Remove a bunch of extra columns
#Action code is I for all data. No need to keep this
QData$Action.Code<-NULL

#Uncertainty, AMDL, and these qualifiers are NA for all data
QData$Uncertainty<-NULL
QData$Alternate.Method.Detectable.Limit<-NULL
QData$Qualifier...10<-NULL
QData$Qualifier...9<-NULL
QData$Qualifier...8<-NULL
QData$Qualifier...7<-NULL
QData$Qualifier...6<-NULL
QData$Qualifier...5<-NULL
QData$Qualifier...4<-NULL
QData$Qualifier...3<-NULL
```

At this point in the process, many of the factors are listed as being of "character" data type. In order to more effectively do analysis, it was necessary to correct this by naming them as factors.

```{r}
#Some of the data is of the wrong type, correct this
QData$X..RD<-factor(QData$X..RD)
QData$Parameter<-factor(QData$Parameter)
QData$Sample.Value<-as.numeric(as.character(QData$Sample.Value))
QData$POC<-factor(QData$POC)
QData$Sample.Duration<-factor(QData$Sample.Duration)
QData$Unit<-factor(QData$Unit)
QData$Method<-factor(QData$Method)
QData$Null.Data.Code<-factor(QData$Null.Data.Code)
QData$Sampling.Frequency<-factor(QData$Sampling.Frequency)
QData$Number.of.Samples<-factor(QData$Number.of.Samples)
```

The data set is now all in the appropriate format and collected into a single dataframe. From this point it can be manipulated and analysed more easily.

###Further Data Manipulation and Cleaning

Currently, the Date attribute for each observation is given as a string of numerals. For instance January 23, 2007 would be listed as 20070123. In order to be able to analyze the data on a monthly or yearly basis more easily, the year and month were extracted into their own separate factors. Furthermore, R is capable of handling Dates in a native format, which will track details such as the day of the week, the number of days in each month, and even leap years. Therefore, the Date and hour variables were combined into a new variable called DateTime.

```{r, cache=TRUE}
#Create Year factor
QData$year<-factor(substr(as.character(QData$Date),1,4))

#Create Month Factor
QData$month<-factor(substr(as.character(QData$Date),5,6))

#Paste together Date and hour to get DateTime and make it a POSIXct type
QData$DateTime<-paste(QData$Date,QData$Start.Time,sep=":")
QData$DateTime<-as.POSIXct(QData$DateTime,format="%Y%m%d:%R")
```

The counties in which observation sites are located are given using the site Federal Information Processing Standards (**FIPS**) code. While this is precise, it is not very human-readable. Therefore, for the purposes of analysis and visualization it is useful to include the County Names. The following code loads a data-frame relating FIPS codes to County names and merges the information into the QData data frame.

```{r,cache=TRUE}
#Paste together State.Code, County.Code, and Site.ID to create the full Site ID and save it as a factor back in Site ID
QData$Site.ID<-factor(paste(QData$State.Code,QData$County.Code,QData$Site.ID,sep=""))
QData$State.Code <- NULL

#Create Readable county names
fips<-read.csv("FIPS.csv",header=FALSE,colClasses=c("NULL","NULL","factor","factor","NULL"),
               col.names=c("State","StateCode","CountyCode","CountyName","ClassCode"))

# merging it with the Data
QData<-merge(QData,fips,by.x="County.Code",by.y="CountyCode",all.x=TRUE)
QData$CountyName <- factor(QData$CountyName)
```

Many of the sites where data was collected have common names in addition to their Site ID. For the purposes of analysis and visualization, it is also useful to give the human-readable common names. The following code loads a data-frame relating the Site ID and the Site Names and merges the information into the QData data frame. This table also includes information about whether the sites are located in the Metro Atlanta Area or not.

```{r,cache=TRUE}
#Add Sitenames and Location type
SiteLookup<-read.csv("Sites2.csv",sep=",",header=T)
QData<-merge(QData,SiteLookup,by.x="Site.ID",by.y="Site.ID",all.x=TRUE)
QData$MetroAtlanta<-gsub("no",QData$MetroAtlanta,ignore.case=T,replacement="State")
QData$MetroAtlanta<-gsub("yes",QData$MetroAtlanta,ignore.case=T,replacement="Metro-Atlanta")
```

This refined data can then be saved into another RData file so it is easy to load the cleaned dataset for further analysis.

```{r,eval=FALSE}
save(QData,file="CleanedQData.RData")
```

###Splitting Data by Criteria Pollutant

Now that the entire dataset has gone through an initial cleaning, it is useful to split it up into criterion pollutants which can be analyzed individually. Each of these files can be saved as RData files for easy loading as well.

```{r,eval=FALSE}
#Create separate files for each criteria pollutant
#Lead has two parameter codes '12128' and '14129' Extract them both and save in LeadData
Lead<-QData[which(QData$Parameter=='12128'|QData$Parameter=='14129'),]
save(Lead,file="LeadData.RData")

PM2.5<-QData[which(QData$Parameter=='88101'),]
save(PM2.5,file="PM25.RData")

SO2<-QData[which(QData$Parameter=='42401'),]
save(SO2,file="SO2.RData")

NO2<-QData[which(QData$Parameter=='42602'),]
save(NO2,file="NO2.RData")

O3<-QData[which(QData$Parameter=='44201'),]
save(O3,file="O3.RData")

rm(QData)
```

